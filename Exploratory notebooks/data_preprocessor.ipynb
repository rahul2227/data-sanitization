{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586b928b2ef449b5",
   "metadata": {},
   "source": [
    "# Data Preprocessing Module MVP for Exploration\n",
    "\n",
    "In this module I will be primarily focusing on the basics of preprocessing textual based data.\n",
    "- Text Cleaning and Normalization\n",
    "- Tokenization\n",
    "- Deduplication\n",
    "- Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "id": "85c5659532873afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:00.338521Z",
     "start_time": "2025-03-28T15:23:57.330659Z"
    }
   },
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Used for normalization and text cleaning\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# For tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rahul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/rahul/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ba218a83523211e5",
   "metadata": {},
   "source": [
    "### Text Cleaning and normalization\n",
    "This step insures consistency in text input, reducing noise that can adversely affectlater preprocessing or model traning.\n",
    "\n",
    "using the opensource [wikitext-103-raw-v1 dataset from huggingface](https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1)"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:05.631043Z",
     "start_time": "2025-03-28T15:24:00.340985Z"
    }
   },
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"train\")\n",
    "df_wiki = pd.DataFrame(dataset)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "ae75f43b02ae78f1",
   "metadata": {},
   "source": [
    "Because the size of the preprocessed dataset was too large, I am capping the dataset size to 25 GB for initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "60d8cd045f45bd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:05.874814Z",
     "start_time": "2025-03-28T15:24:05.700020Z"
    }
   },
   "source": [
    "# Calculate the byte size of each text entry\n",
    "df_wiki['text_size'] = df_wiki['text'].apply(lambda x: len(x.encode('utf-8')))\n",
    "\n",
    "# Compute cumulative size (in bytes)\n",
    "df_wiki['cum_size'] = df_wiki['text_size'].cumsum()\n",
    "\n",
    "# Define maximum allowed size: 25GB in bytes\n",
    "max_bytes = 25 * 1024 * 1024 * 1024\n",
    "\n",
    "# Filter the DataFrame to keep rows until we reach the cap\n",
    "df_wiki_capped = df_wiki[df_wiki['cum_size'] <= max_bytes].copy()\n",
    "print(f\"Original dataset rows: {len(df_wiki)}\")\n",
    "print(f\"Rows retained (capped to ~25GB): {len(df_wiki_capped)}\")\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_wiki_capped.drop(columns=['text_size', 'cum_size'], inplace=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset rows: 29567\n",
      "Rows retained (capped to ~25GB): 29567\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "40cc69a66e3d823f",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "58c7a8419645bb24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:05.884469Z",
     "start_time": "2025-03-28T15:24:05.879774Z"
    }
   },
   "source": [
    "df_wiki_capped.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text\n",
       "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...\n",
       "1  = Tower Building of the Little Rock Arsenal =\\...\n",
       "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...\n",
       "3  = Gambia women's national football team =\\nThe...\n",
       "4  = Plain maskray =\\nThe plain maskray or brown ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Valkyria Chronicles III =\\nSenjō no Valkyria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal =\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= Cicely Mary Barker =\\nCicely Mary Barker (28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>= Gambia women's national football team =\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= Plain maskray =\\nThe plain maskray or brown ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "29d842394dca08e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:05.904926Z",
     "start_time": "2025-03-28T15:24:05.902175Z"
    }
   },
   "source": [
    "df_wiki_capped.tail()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                    text\n",
       "29562  = Si Una Vez =\\n\"Si Una Vez\" (English: If I On...\n",
       "29563  = Sicklefin lemon shark =\\nThe sicklefin lemon...\n",
       "29564  = Flammulated flycatcher =\\nThe flammulated fl...\n",
       "29565  = Ontario Highway 89 =\\nKing's Highway 89, com...\n",
       "29566  = Luke Smith (writer) =\\nLuke Michael Smith is..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29562</th>\n",
       "      <td>= Si Una Vez =\\n\"Si Una Vez\" (English: If I On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>= Sicklefin lemon shark =\\nThe sicklefin lemon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>= Flammulated flycatcher =\\nThe flammulated fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29565</th>\n",
       "      <td>= Ontario Highway 89 =\\nKing's Highway 89, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29566</th>\n",
       "      <td>= Luke Smith (writer) =\\nLuke Michael Smith is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a15e684eba5f5845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:05.925848Z",
     "start_time": "2025-03-28T15:24:05.924220Z"
    }
   },
   "source": [
    "# Basic info\n",
    "print(\"Dataset columns:\", df_wiki_capped.columns)\n",
    "print(\"Dataset shape:\", df_wiki_capped.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['text'], dtype='object')\n",
      "Dataset shape: (29567, 1)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "73f6a5bba99f51d8",
   "metadata": {},
   "source": [
    "#### Data cleaning exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e5e4b85a0259e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:05.961681Z",
     "start_time": "2025-03-28T15:24:05.951401Z"
    }
   },
   "source": [
    "# text length for each entry\n",
    "df_wiki_capped['text_length'] = df_wiki_capped['text'].apply(lambda x: len(x))\n",
    "print(\"Text lenght stats:\")\n",
    "print(df_wiki_capped['text_length'].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lenght stats:\n",
      "count     29567.000000\n",
      "mean      17537.078161\n",
      "std       14555.364685\n",
      "min          16.000000\n",
      "25%        7750.000000\n",
      "50%       12994.000000\n",
      "75%       22721.500000\n",
      "max      140098.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3e1f145624cd63e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:06.510543Z",
     "start_time": "2025-03-28T15:24:05.999199Z"
    }
   },
   "source": [
    "# Lines with problematic formatting\n",
    "problematic = df_wiki_capped[df_wiki_capped['text'].str.contains(r'[\"]{1,}', na=False)]\n",
    "print(\"Rows with potential quotation issues: \") # decided to do this because of an table formatting error while doing df_wiki.head() step in the IDE.\n",
    "# The error was: “Unterminated quoted field at end of CSV line”\n",
    "print(problematic.head(5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with potential quotation issues: \n",
      "                                                text  text_length\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297\n",
      "1  = Tower Building of the Little Rock Arsenal =\\...        20770\n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...        15371\n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...         6695\n",
      "5  = 2011 – 12 Columbus Blue Jackets season =\\nTh...        17189\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "6f9ccbd79adfc944",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "cleaning function to:\n",
    "- Remove HTML tags\n",
    "- Normalize Unicode to standardize characters.\n",
    "- Convert text to lowercase and remove accent marks\n",
    "- Remove non-UTF characters and extra whitespaces.\n",
    "\n",
    "This should make the text more uniform for intial tokenization."
   ]
  },
  {
   "cell_type": "code",
   "id": "e94086e91b468bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:06.593676Z",
     "start_time": "2025-03-28T15:24:06.591905Z"
    }
   },
   "source": [
    "def normalize_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Normalize Unicode (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Lowercase conversion and accent stripping\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    # Remove non-UTF characters and extra whitespace\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "8e2f052097384a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:24:39.078060Z",
     "start_time": "2025-03-28T15:24:06.602410Z"
    }
   },
   "source": [
    "# Applying normalizations\n",
    "df_wiki_capped['cleaned_text'] = df_wiki_capped['text'].apply(normalize_text)\n",
    "print(\"cleaned sample:\")\n",
    "print(df_wiki_capped[['text', 'cleaned_text']].head(5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned sample:\n",
      "                                                text  \\\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...   \n",
      "1  = Tower Building of the Little Rock Arsenal =\\...   \n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...   \n",
      "3  = Gambia women's national football team =\\nThe...   \n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "1  = tower building of the little rock arsenal = ...  \n",
      "2  = cicely mary barker = cicely mary barker (28 ...  \n",
      "3  = gambia women's national football team = the ...  \n",
      "4  = plain maskray = the plain maskray or brown s...  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ea545470ae7e304c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenizing the cleaned data using Hugging Face fast tokenizer for essential downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4110d505c06390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:26:37.052383Z",
     "start_time": "2025-03-28T15:24:39.083083Z"
    }
   },
   "source": [
    "# Tokenizer initialization\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2', use_fast=True)\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.tokenize(text, truncation=True, max_length=1024) # changes made because of warning\n",
    "    return tokens\n",
    "\n",
    "df_wiki_capped['tokens'] = df_wiki_capped['cleaned_text'].apply(tokenize_text)\n",
    "print(\"tokenized sample:\")\n",
    "print(df_wiki_capped[['cleaned_text', 'tokens']].head(2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "1  = tower building of the little rock arsenal = ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [=, Ġv, alky, ria, Ġchron, icles, Ġiii, Ġ=, Ġs...  \n",
      "1  [=, Ġtower, Ġbuilding, Ġof, Ġthe, Ġlittle, Ġro...  \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "90b1e93a81f633e",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "Deduplication is the process of removing duplicates or near-duplicates to avoid redundancy in the dataset. For now I am choosing to remove the exact-matches."
   ]
  },
  {
   "cell_type": "code",
   "id": "319c48482dcdfe24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:26:37.197965Z",
     "start_time": "2025-03-28T15:26:37.064738Z"
    }
   },
   "source": [
    "def duplicate_texts(df, text_column = 'cleaned_text'):\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    return df\n",
    "\n",
    "df_wiki_capped_unique = duplicate_texts(df_wiki_capped)\n",
    "print(\"Number of rows after deduplication:\", len(df_wiki_capped_unique))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after deduplication: 29116\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "1892177465583057",
   "metadata": {},
   "source": [
    "### Data Segmentation\n",
    "segmenting the cleaned text into sentences for finer analysis using NLTK's sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "id": "6aefa5ac5b9af218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:26:37.202030Z",
     "start_time": "2025-03-28T15:26:37.200297Z"
    }
   },
   "source": [
    "def segment_text(text, mode='sentence', fixed_token_length=100):\n",
    "    if mode == 'sentence':\n",
    "        segments = sent_tokenize(text)\n",
    "    elif mode == 'fixed':\n",
    "        tokens = tokenize_text(text)\n",
    "        segments = [' '.join(tokens[i:i+fixed_token_length]) for i in range(0, len(tokens), fixed_token_length)]\n",
    "    else:\n",
    "        segments = [text]\n",
    "    return segments"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "4a5e5774968bdb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:26:37.216821Z",
     "start_time": "2025-03-28T15:26:37.214805Z"
    }
   },
   "source": [
    "def segment_dataframe(df, text_column='cleaned_text', mode='sentence'):\n",
    "    df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n",
    "    df_segmented = df.explode('segments')\n",
    "    return df_segmented"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "9298c2ac5db48788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:27:02.076503Z",
     "start_time": "2025-03-28T15:26:37.220021Z"
    }
   },
   "source": [
    "df_segmented = segment_dataframe(df_wiki_capped_unique, text_column='cleaned_text', mode='sentence')\n",
    "print(\"Segmented data sample:\")\n",
    "print(df_segmented[['cleaned_text', 'segments']].head(5))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/w0qrc74d6sj7fjdwc593xcqc0000gn/T/ipykernel_6873/2733018757.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented data sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "\n",
      "                                            segments  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "0  valkyria of the battlefield 3), commonly refer...  \n",
      "0  released in january 2011 in japan, it is the t...  \n",
      "0  employing the same fusion of tactical and real...  \n",
      "0  the game began development in 2010, carrying o...  \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "66e8bc80391c8886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:27:02.086793Z",
     "start_time": "2025-03-28T15:27:02.084974Z"
    }
   },
   "source": [
    "len(df_segmented)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3414617"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f1f2674081e7cee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:27:02.095833Z",
     "start_time": "2025-03-28T15:27:02.094096Z"
    }
   },
   "source": [
    "# Further capping the data\n",
    "# Calculate total number of segmented rows\n",
    "total_rows = len(df_segmented)\n",
    "one_eighteenth_rows = int(total_rows / 18)\n",
    "print(f\"Total segmented rows: {total_rows}\")\n",
    "print(f\"Keeping only one third: {one_eighteenth_rows} rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segmented rows: 3414617\n",
      "Keeping only one third: 189700 rows\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "5e7a0f5e1a28035d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:27:02.118070Z",
     "start_time": "2025-03-28T15:27:02.102927Z"
    }
   },
   "source": [
    "# Select the first one third of the rows\n",
    "df_segmented_subset = df_segmented.iloc[:one_eighteenth_rows].copy()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "7ec0e3e70a0131d1",
   "metadata": {},
   "source": [
    "### Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3cb944bb1915ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:28:32.035163Z",
     "start_time": "2025-03-28T15:27:02.125889Z"
    }
   },
   "source": [
    "output_dir = \"../data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"preprocessed_wikitext103_subset.csv\")\n",
    "\n",
    "# Save the segmented DataFrame to the specified CSV file\n",
    "df_segmented_subset.to_csv(output_file, index=False)\n",
    "print(f\"Subset of preprocessed data saved to {output_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of preprocessed data saved to ../data/preprocessed_wikitext103_subset.csv\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "66f7416d5ca3108d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:32:16.356996Z",
     "start_time": "2025-03-28T15:28:32.051074Z"
    }
   },
   "source": [
    "# testing by loading the data\n",
    "\n",
    "# Define the file path in the \"data\" directory\n",
    "data_file = os.path.join(\"../data\", \"preprocessed_wikitext103_subset.csv\")\n",
    "\n",
    "# Load the CSV file with a safe option to skip problematic lines if any exist\n",
    "df_loaded = pd.read_csv(data_file, on_bad_lines='skip', engine='python')\n",
    "\n",
    "# Display a sample of the loaded data and its dimensions\n",
    "print(\"Loaded data sample:\")\n",
    "print(df_loaded.head())\n",
    "print(\"\\nShape of loaded data:\", df_loaded.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data sample:\n",
      "                                                text  text_length  \\\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "1  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "2  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "3  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "4  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "1  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "2  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "3  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "4  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  ['=', 'Ġv', 'alky', 'ria', 'Ġchron', 'icles', ...   \n",
      "1  ['=', 'Ġv', 'alky', 'ria', 'Ġchron', 'icles', ...   \n",
      "2  ['=', 'Ġv', 'alky', 'ria', 'Ġchron', 'icles', ...   \n",
      "3  ['=', 'Ġv', 'alky', 'ria', 'Ġchron', 'icles', ...   \n",
      "4  ['=', 'Ġv', 'alky', 'ria', 'Ġchron', 'icles', ...   \n",
      "\n",
      "                                            segments  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "1  valkyria of the battlefield 3), commonly refer...  \n",
      "2  released in january 2011 in japan, it is the t...  \n",
      "3  employing the same fusion of tactical and real...  \n",
      "4  the game began development in 2010, carrying o...  \n",
      "\n",
      "Shape of loaded data: (189700, 5)\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
