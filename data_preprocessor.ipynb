{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586b928b2ef449b5",
   "metadata": {},
   "source": [
    "# Data Preprocessing Module MVP for Exploration\n",
    "\n",
    "In this module I will be primarily focusing on the basics of preprocessing textual based data.\n",
    "- Text Cleaning and Normalization\n",
    "- Tokenization\n",
    "- Deduplication\n",
    "- Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c5659532873afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:26.912201Z",
     "start_time": "2025-03-18T23:45:24.269761Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ROSHAL\n",
      "[nltk_data]     CARDOZA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\ROSHAL\n",
      "[nltk_data]     CARDOZA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Used for normalization and text cleaning\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# For tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba218a83523211e5",
   "metadata": {},
   "source": [
    "### Text Cleaning and normalization\n",
    "This step insures consistency in text input, reducing noise that can adversely affectlater preprocessing or model traning.\n",
    "\n",
    "using the opensource [wikitext-103-raw-v1 dataset from huggingface](https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:35.141273Z",
     "start_time": "2025-03-18T23:45:26.914671Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"train\")\n",
    "df_wiki = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75f43b02ae78f1",
   "metadata": {},
   "source": [
    "Because the size of the preprocessed dataset was too large, I am capping the dataset size to 25 GB for initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d8cd045f45bd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:35.372289Z",
     "start_time": "2025-03-18T23:45:35.203064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset rows: 29567\n",
      "Rows retained (capped to ~25GB): 29567\n"
     ]
    }
   ],
   "source": [
    "# Calculate the byte size of each text entry\n",
    "df_wiki['text_size'] = df_wiki['text'].apply(lambda x: len(x.encode('utf-8')))\n",
    "\n",
    "# Compute cumulative size (in bytes)\n",
    "df_wiki['cum_size'] = df_wiki['text_size'].cumsum()\n",
    "\n",
    "# Define maximum allowed size: 25GB in bytes\n",
    "max_bytes = 25 * 1024 * 1024 * 1024\n",
    "\n",
    "# Filter the DataFrame to keep rows until we reach the cap\n",
    "df_wiki_capped = df_wiki[df_wiki['cum_size'] <= max_bytes].copy()\n",
    "print(f\"Original dataset rows: {len(df_wiki)}\")\n",
    "print(f\"Rows retained (capped to ~25GB): {len(df_wiki_capped)}\")\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_wiki_capped.drop(columns=['text_size', 'cum_size'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc69a66e3d823f",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c7a8419645bb24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:35.382023Z",
     "start_time": "2025-03-18T23:45:35.377278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Valkyria Chronicles III =\\nSenjō no Valkyria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal =\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= Cicely Mary Barker =\\nCicely Mary Barker (28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>= Gambia women's national football team =\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= Plain maskray =\\nThe plain maskray or brown ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...\n",
       "1  = Tower Building of the Little Rock Arsenal =\\...\n",
       "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...\n",
       "3  = Gambia women's national football team =\\nThe...\n",
       "4  = Plain maskray =\\nThe plain maskray or brown ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_capped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d842394dca08e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:35.435750Z",
     "start_time": "2025-03-18T23:45:35.432644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29562</th>\n",
       "      <td>= Si Una Vez =\\n\"Si Una Vez\" (English: If I On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>= Sicklefin lemon shark =\\nThe sicklefin lemon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>= Flammulated flycatcher =\\nThe flammulated fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29565</th>\n",
       "      <td>= Ontario Highway 89 =\\nKing's Highway 89, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29566</th>\n",
       "      <td>= Luke Smith (writer) =\\nLuke Michael Smith is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "29562  = Si Una Vez =\\n\"Si Una Vez\" (English: If I On...\n",
       "29563  = Sicklefin lemon shark =\\nThe sicklefin lemon...\n",
       "29564  = Flammulated flycatcher =\\nThe flammulated fl...\n",
       "29565  = Ontario Highway 89 =\\nKing's Highway 89, com...\n",
       "29566  = Luke Smith (writer) =\\nLuke Michael Smith is..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki_capped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15e684eba5f5845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:35.457373Z",
     "start_time": "2025-03-18T23:45:35.455626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['text'], dtype='object')\n",
      "Dataset shape: (29567, 1)\n"
     ]
    }
   ],
   "source": [
    "# Basic info\n",
    "print(\"Dataset columns:\", df_wiki_capped.columns)\n",
    "print(\"Dataset shape:\", df_wiki_capped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6a5bba99f51d8",
   "metadata": {},
   "source": [
    "#### Data cleaning exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5e4b85a0259e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:35.528728Z",
     "start_time": "2025-03-18T23:45:35.516478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lenght stats:\n",
      "count     29567.000000\n",
      "mean      17537.078161\n",
      "std       14555.364685\n",
      "min          16.000000\n",
      "25%        7750.000000\n",
      "50%       12994.000000\n",
      "75%       22721.500000\n",
      "max      140098.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# text length for each entry\n",
    "df_wiki_capped['text_length'] = df_wiki_capped['text'].apply(lambda x: len(x))\n",
    "print(\"Text lenght stats:\")\n",
    "print(df_wiki_capped['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1f145624cd63e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:36.076208Z",
     "start_time": "2025-03-18T23:45:35.558552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with potential quotation issues: \n",
      "                                                text  text_length\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297\n",
      "1  = Tower Building of the Little Rock Arsenal =\\...        20770\n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...        15371\n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...         6695\n",
      "5  = 2011 – 12 Columbus Blue Jackets season =\\nTh...        17189\n"
     ]
    }
   ],
   "source": [
    "# Lines with problematic formatting\n",
    "problematic = df_wiki_capped[df_wiki_capped['text'].str.contains(r'[\"]{1,}', na=False)]\n",
    "print(\"Rows with potential quotation issues: \") # decided to do this because of an table formatting error while doing df_wiki.head() step in the IDE.\n",
    "# The error was: “Unterminated quoted field at end of CSV line”\n",
    "print(problematic.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ccbd79adfc944",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "cleaning function to:\n",
    "- Remove HTML tags\n",
    "- Normalize Unicode to standardize characters.\n",
    "- Convert text to lowercase and remove accent marks\n",
    "- Remove non-UTF characters and extra whitespaces.\n",
    "\n",
    "This should make the text more uniform for intial tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94086e91b468bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:45:36.080623Z",
     "start_time": "2025-03-18T23:45:36.078793Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Normalize Unicode (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Lowercase conversion and accent stripping\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    # Remove non-UTF characters and extra whitespace\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2f052097384a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:46:08.516850Z",
     "start_time": "2025-03-18T23:45:36.092600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned sample:\n",
      "                                                text  \\\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...   \n",
      "1  = Tower Building of the Little Rock Arsenal =\\...   \n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...   \n",
      "3  = Gambia women's national football team =\\nThe...   \n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "1  = tower building of the little rock arsenal = ...  \n",
      "2  = cicely mary barker = cicely mary barker (28 ...  \n",
      "3  = gambia women's national football team = the ...  \n",
      "4  = plain maskray = the plain maskray or brown s...  \n"
     ]
    }
   ],
   "source": [
    "# Applying normalizations\n",
    "df_wiki_capped['cleaned_text'] = df_wiki_capped['text'].apply(normalize_text)\n",
    "print(\"cleaned sample:\")\n",
    "print(df_wiki_capped[['text', 'cleaned_text']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea545470ae7e304c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenizing the cleaned data using Hugging Face fast tokenizer for essential downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4110d505c06390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:48:05.893018Z",
     "start_time": "2025-03-18T23:46:08.521910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4206 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "1  = tower building of the little rock arsenal = ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [=, val, ##ky, ##ria, chronicles, iii, =, sen,...  \n",
      "1  [=, tower, building, of, the, little, rock, ar...  \n"
     ]
    }
   ],
   "source": [
    "# Tokenizer initialization\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df_wiki_capped['tokens'] = df_wiki_capped['cleaned_text'].apply(tokenize_text)\n",
    "print(\"tokenized sample:\")\n",
    "print(df_wiki_capped[['cleaned_text', 'tokens']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1e93a81f633e",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "Deduplication is the process of removing duplicates or near-duplicates to avoid redundancy in the dataset. For now I am choosing to remove the exact-matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "319c48482dcdfe24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:48:06.080075Z",
     "start_time": "2025-03-18T23:48:05.922253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after deduplication: 29116\n"
     ]
    }
   ],
   "source": [
    "def duplicate_texts(df, text_column = 'cleaned_text'):\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    return df\n",
    "\n",
    "df_wiki_capped_unique = duplicate_texts(df_wiki_capped)\n",
    "print(\"Number of rows after deduplication:\", len(df_wiki_capped_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892177465583057",
   "metadata": {},
   "source": [
    "### Data Segmentation\n",
    "segmenting the cleaned text into sentences for finer analysis using NLTK's sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aefa5ac5b9af218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:48:06.095696Z",
     "start_time": "2025-03-18T23:48:06.093877Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_text(text, mode='sentence', fixed_token_length=100):\n",
    "    if mode == 'sentence':\n",
    "        segments = sent_tokenize(text)\n",
    "    elif mode == 'fixed':\n",
    "        tokens = tokenize_text(text)\n",
    "        segments = [' '.join(tokens[i:i+fixed_token_length]) for i in range(0, len(tokens), fixed_token_length)]\n",
    "    else:\n",
    "        segments = [text]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5e5774968bdb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:48:06.109189Z",
     "start_time": "2025-03-18T23:48:06.107536Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_dataframe(df, text_column='cleaned_text', mode='sentence'):\n",
    "    df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n",
    "    df_segmented = df.explode('segments')\n",
    "    return df_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9298c2ac5db48788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:48:31.391656Z",
     "start_time": "2025-03-18T23:48:06.112507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROSHAL CARDOZA\\AppData\\Local\\Temp\\ipykernel_22268\\4147776411.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented data sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "\n",
      "                                            segments  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "0  valkyria of the battlefield 3), commonly refer...  \n",
      "0  released in january 2011 in japan, it is the t...  \n",
      "0  employing the same fusion of tactical and real...  \n",
      "0  the game began development in 2010, carrying o...  \n"
     ]
    }
   ],
   "source": [
    "df_segmented = segment_dataframe(df_wiki_capped_unique, text_column='cleaned_text', mode='sentence')\n",
    "print(\"Segmented data sample:\")\n",
    "print(df_segmented[['cleaned_text', 'segments']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66e8bc80391c8886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T23:48:31.415662Z",
     "start_time": "2025-03-18T23:48:31.413589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3414617"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f2674081e7cee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T00:09:48.664705Z",
     "start_time": "2025-03-19T00:09:48.660800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segmented rows: 3414617\n",
      "Keeping only one third: 189700 rows\n"
     ]
    }
   ],
   "source": [
    "# Further capping the data\n",
    "# Calculate total number of segmented rows\n",
    "total_rows = len(df_segmented)\n",
    "one_eighteenth_rows = int(total_rows / 18)\n",
    "print(f\"Total segmented rows: {total_rows}\")\n",
    "print(f\"Keeping only one third: {one_eighteenth_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e7a0f5e1a28035d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T00:09:48.880244Z",
     "start_time": "2025-03-19T00:09:48.844723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the first one third of the rows\n",
    "df_segmented_subset = df_segmented.iloc[:one_eighteenth_rows].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0e3e70a0131d1",
   "metadata": {},
   "source": [
    "### Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3cb944bb1915ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T00:12:39.149287Z",
     "start_time": "2025-03-19T00:09:49.314225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of preprocessed data saved to data\\preprocessed_wikitext103_subset.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"preprocessed_wikitext103_subset.csv\")\n",
    "\n",
    "# Save the segmented DataFrame to the specified CSV file\n",
    "df_segmented_subset.to_csv(output_file, index=False)\n",
    "print(f\"Subset of preprocessed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66f7416d5ca3108d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T00:21:21.913443Z",
     "start_time": "2025-03-19T00:15:51.882747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data sample:\n",
      "                                                text  text_length  \\\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "1  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "2  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "3  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "4  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "1  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "2  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "3  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "4  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  ['=', 'val', '##ky', '##ria', 'chronicles', 'i...   \n",
      "1  ['=', 'val', '##ky', '##ria', 'chronicles', 'i...   \n",
      "2  ['=', 'val', '##ky', '##ria', 'chronicles', 'i...   \n",
      "3  ['=', 'val', '##ky', '##ria', 'chronicles', 'i...   \n",
      "4  ['=', 'val', '##ky', '##ria', 'chronicles', 'i...   \n",
      "\n",
      "                                            segments  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "1  valkyria of the battlefield 3), commonly refer...  \n",
      "2  released in january 2011 in japan, it is the t...  \n",
      "3  employing the same fusion of tactical and real...  \n",
      "4  the game began development in 2010, carrying o...  \n",
      "\n",
      "Shape of loaded data: (186070, 5)\n"
     ]
    }
   ],
   "source": [
    "# testing by loading the data\n",
    "\n",
    "# Define the file path in the \"data\" directory\n",
    "data_file = os.path.join(\"data\", \"preprocessed_wikitext103_subset.csv\")\n",
    "\n",
    "# Load the CSV file with a safe option to skip problematic lines if any exist\n",
    "df_loaded = pd.read_csv(data_file, on_bad_lines='skip', engine='python')\n",
    "\n",
    "# Display a sample of the loaded data and its dimensions\n",
    "print(\"Loaded data sample:\")\n",
    "print(df_loaded.head())\n",
    "print(\"\\nShape of loaded data:\", df_loaded.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
