{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing Module MVP for Exploration\n",
    "\n",
    "In this module I will be primarily focusing on the basics of preprocessing textual based data.\n",
    "- Text Cleaning and Normalization\n",
    "- Tokenization\n",
    "- Deduplication\n",
    "- Segmentation"
   ],
   "id": "586b928b2ef449b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:34.985332Z",
     "start_time": "2025-03-18T22:12:33.411214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Used for normalization and text cleaning\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# For tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "85c5659532873afe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahul/miniconda3/envs/data-sanitization/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/rahul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/rahul/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Text Cleaning and normalization\n",
    "This step insures consistency in text input, reducing noise that can adversely affectlater preprocessing or model traning.\n",
    "\n",
    "using the opensource [wikitext-103-raw-v1 dataset from huggingface](https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1)"
   ],
   "id": "ba218a83523211e5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:39.235502Z",
     "start_time": "2025-03-18T22:12:34.990836Z"
    }
   },
   "source": [
    "# Loading the dataset\n",
    "dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"train\")\n",
    "# converting it into a Data frame for easier processing\n",
    "df_wiki = pd.DataFrame(dataset)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Exploration",
   "id": "40cc69a66e3d823f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:39.302487Z",
     "start_time": "2025-03-18T22:12:39.298395Z"
    }
   },
   "cell_type": "code",
   "source": "df_wiki.head()",
   "id": "58c7a8419645bb24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text\n",
       "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...\n",
       "1  = Tower Building of the Little Rock Arsenal =\\...\n",
       "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...\n",
       "3  = Gambia women's national football team =\\nThe...\n",
       "4  = Plain maskray =\\nThe plain maskray or brown ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Valkyria Chronicles III =\\nSenjō no Valkyria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal =\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= Cicely Mary Barker =\\nCicely Mary Barker (28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>= Gambia women's national football team =\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= Plain maskray =\\nThe plain maskray or brown ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:39.364075Z",
     "start_time": "2025-03-18T22:12:39.359488Z"
    }
   },
   "cell_type": "code",
   "source": "df_wiki.tail()",
   "id": "29d842394dca08e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                    text\n",
       "29562  = Si Una Vez =\\n\"Si Una Vez\" (English: If I On...\n",
       "29563  = Sicklefin lemon shark =\\nThe sicklefin lemon...\n",
       "29564  = Flammulated flycatcher =\\nThe flammulated fl...\n",
       "29565  = Ontario Highway 89 =\\nKing's Highway 89, com...\n",
       "29566  = Luke Smith (writer) =\\nLuke Michael Smith is..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29562</th>\n",
       "      <td>= Si Una Vez =\\n\"Si Una Vez\" (English: If I On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>= Sicklefin lemon shark =\\nThe sicklefin lemon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>= Flammulated flycatcher =\\nThe flammulated fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29565</th>\n",
       "      <td>= Ontario Highway 89 =\\nKing's Highway 89, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29566</th>\n",
       "      <td>= Luke Smith (writer) =\\nLuke Michael Smith is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:39.425359Z",
     "start_time": "2025-03-18T22:12:39.422783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic info\n",
    "print(\"Dataset columns:\", df_wiki.columns)\n",
    "print(\"Dataset shape:\", df_wiki.shape)"
   ],
   "id": "a15e684eba5f5845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['text'], dtype='object')\n",
      "Dataset shape: (29567, 1)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data cleaning exploration",
   "id": "73f6a5bba99f51d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:39.541910Z",
     "start_time": "2025-03-18T22:12:39.525850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text length for each entry\n",
    "df_wiki['text_length'] = df_wiki['text'].apply(lambda x: len(x))\n",
    "print(\"Text lenght stats:\")\n",
    "print(df_wiki['text_length'].describe())"
   ],
   "id": "8e5e4b85a0259e88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lenght stats:\n",
      "count     29567.000000\n",
      "mean      17537.078161\n",
      "std       14555.364685\n",
      "min          16.000000\n",
      "25%        7750.000000\n",
      "50%       12994.000000\n",
      "75%       22721.500000\n",
      "max      140098.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:40.149202Z",
     "start_time": "2025-03-18T22:12:39.592435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lines with problematic formatting\n",
    "problematic = df_wiki[df_wiki['text'].str.contains(r'[\"]{1,}', na=False)]\n",
    "print(\"Rows with potential quotation issues: \") # decided to do this because of an table formatting error while doing df_wiki.head() step in the IDE.\n",
    "# The error was: “Unterminated quoted field at end of CSV line”\n",
    "print(problematic.head(5))"
   ],
   "id": "3e1f145624cd63e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with potential quotation issues: \n",
      "                                                text  text_length\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297\n",
      "1  = Tower Building of the Little Rock Arsenal =\\...        20770\n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...        15371\n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...         6695\n",
      "5  = 2011 – 12 Columbus Blue Jackets season =\\nTh...        17189\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Text Cleaning and Normalization\n",
    "cleaning function to:\n",
    "- Remove HTML tags\n",
    "- Normalize Unicode to standardize characters.\n",
    "- Convert text to lowercase and remove accent marks\n",
    "- Remove non-UTF characters and extra whitespaces.\n",
    "\n",
    "This should make the text more uniform for intial tokenization."
   ],
   "id": "6f9ccbd79adfc944"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:12:40.154259Z",
     "start_time": "2025-03-18T22:12:40.152565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Normalize Unicode (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Lowercase conversion and accent stripping\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    # Remove non-UTF characters and extra whitespace\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ],
   "id": "e94086e91b468bdd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:13:13.719550Z",
     "start_time": "2025-03-18T22:12:40.164602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Applying normalizations\n",
    "df_wiki['cleaned_text'] = df_wiki['text'].apply(normalize_text)\n",
    "print(\"cleaned sample:\")\n",
    "print(df_wiki[['text', 'cleaned_text']].head(5))"
   ],
   "id": "8e2f052097384a1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned sample:\n",
      "                                                text  \\\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...   \n",
      "1  = Tower Building of the Little Rock Arsenal =\\...   \n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...   \n",
      "3  = Gambia women's national football team =\\nThe...   \n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "1  = tower building of the little rock arsenal = ...  \n",
      "2  = cicely mary barker = cicely mary barker (28 ...  \n",
      "3  = gambia women's national football team = the ...  \n",
      "4  = plain maskray = the plain maskray or brown s...  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "Tokenizing the cleaned data using Hugging Face fast tokenizer for essential downstream processing"
   ],
   "id": "ea545470ae7e304c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:15:12.653857Z",
     "start_time": "2025-03-18T22:13:13.725845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer initialization\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df_wiki['tokens'] = df_wiki['cleaned_text'].apply(tokenize_text)\n",
    "print(\"tokenized sample:\")\n",
    "print(df_wiki[['cleaned_text', 'tokens']].head(2))"
   ],
   "id": "e4110d505c06390",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4206 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "1  = tower building of the little rock arsenal = ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [=, val, ##ky, ##ria, chronicles, iii, =, sen,...  \n",
      "1  [=, tower, building, of, the, little, rock, ar...  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deduplication\n",
    "Deduplication is the process of removing duplicates or near-duplicates to avoid redundancy in the dataset. For now I am choosing to remove the exact-matches."
   ],
   "id": "90b1e93a81f633e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:15:13.068437Z",
     "start_time": "2025-03-18T22:15:12.774885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def duplicate_texts(df, text_column = 'cleaned_text'):\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    return df\n",
    "\n",
    "df_wiki_unique = duplicate_texts(df_wiki)\n",
    "print(\"Number of rows after deduplication:\", len(df_wiki_unique))"
   ],
   "id": "319c48482dcdfe24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after deduplication: 29116\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Segmentation\n",
    "segmenting the cleaned text into sentences for finer analysis using NLTK's sentence tokenizer."
   ],
   "id": "1892177465583057"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:15:13.080325Z",
     "start_time": "2025-03-18T22:15:13.078621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def segment_text(text, mode='sentence', fixed_token_length=100):\n",
    "    if mode == 'sentence':\n",
    "        segments = sent_tokenize(text)\n",
    "    elif mode == 'fixed':\n",
    "        tokens = tokenize_text(text)\n",
    "        segments = [' '.join(tokens[i:i+fixed_token_length]) for i in range(0, len(tokens), fixed_token_length)]\n",
    "    else:\n",
    "        segments = [text]\n",
    "    return segments"
   ],
   "id": "6aefa5ac5b9af218",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:15:13.107746Z",
     "start_time": "2025-03-18T22:15:13.106190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def segment_dataframe(df, text_column='cleaned_text', mode='sentence'):\n",
    "    df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n",
    "    df_segmented = df.explode('segments')\n",
    "    return df_segmented"
   ],
   "id": "4a5e5774968bdb35",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T22:15:41.000373Z",
     "start_time": "2025-03-18T22:15:13.145348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_segmented = segment_dataframe(df_wiki_unique, text_column='cleaned_text', mode='sentence')\n",
    "print(\"Segmented data sample:\")\n",
    "print(df_segmented[['cleaned_text', 'segments']].head(5))"
   ],
   "id": "9298c2ac5db48788",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/w0qrc74d6sj7fjdwc593xcqc0000gn/T/ipykernel_2988/2733018757.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented data sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "\n",
      "                                            segments  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "0  valkyria of the battlefield 3), commonly refer...  \n",
      "0  released in january 2011 in japan, it is the t...  \n",
      "0  employing the same fusion of tactical and real...  \n",
      "0  the game began development in 2010, carrying o...  \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving the preprocessed data",
   "id": "7ec0e3e70a0131d1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-18T22:15:41.009710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "output_file = os.path.join(output_dir, \"preprocessed_wikitext103.csv\")\n",
    "\n",
    "# Save the segmented DataFrame to the specified CSV file\n",
    "df_segmented.to_csv(output_file, index=False)\n",
    "print(f\"Preprocessed data saved to {output_file}\")"
   ],
   "id": "e3cb944bb1915ce4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
