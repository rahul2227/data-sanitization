{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586b928b2ef449b5",
   "metadata": {},
   "source": [
    "# Data Preprocessing Module MVP for Exploration\n",
    "\n",
    "In this module I will be primarily focusing on the basics of preprocessing textual based data.\n",
    "- Text Cleaning and Normalization\n",
    "- Tokenization\n",
    "- Deduplication\n",
    "- Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a87551-c87b-4708-84f3-37196d45b1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in c:\\tools\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\tools\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\tools\\anaconda3\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\tools\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\tools\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\roshal cardoza\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\tools\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\tools\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\tools\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\tools\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\tools\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\tools\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\tools\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\tools\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\tools\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\tools\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\tools\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\tools\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\tools\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\tools\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\tools\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\tools\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\tools\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c5659532873afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:20.236405Z",
     "start_time": "2025-03-18T21:28:18.903346Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ROSHAL\n",
      "[nltk_data]     CARDOZA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\ROSHAL\n",
      "[nltk_data]     CARDOZA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Used for normalization and text cleaning\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# For tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba218a83523211e5",
   "metadata": {},
   "source": [
    "### Text Cleaning and normalization\n",
    "This step insures consistency in text input, reducing noise that can adversely affectlater preprocessing or model traning.\n",
    "\n",
    "using the opensource [wikitext-103-raw-v1 dataset from huggingface](https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:23.678310Z",
     "start_time": "2025-03-18T21:28:20.239484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059e9cfa74ee488d8f032e9f840dca00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/802 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROSHAL CARDOZA\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ROSHAL CARDOZA\\.cache\\huggingface\\hub\\datasets--iohadrubin--wikitext-103-raw-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b54a61b7931462fbd2975b0ddcdf5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00002-b755d19de94348c6.parquet:   0%|          | 0.00/148M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9d91eafae8481597ca79093171f0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00001-of-00002-0bf6d0c487c2e75b.parquet:   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5e619837084c35980f0cdb387f7367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-4c013962448951dd.parquet:   0%|          | 0.00/631k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cfe30037c9474682a82471dd885409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-b7859cf6365689a3.parquet:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e34e5ce8660423aa07192ad252fff38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/29567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c832e0cdbcd4868bf950362832ac7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77073ea29124dd5a7d398cf135647df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/62 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"train\")\n",
    "# converting it into a Data frame for easier processing\n",
    "df_wiki = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc69a66e3d823f",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c7a8419645bb24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:23.747514Z",
     "start_time": "2025-03-18T21:28:23.743376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Valkyria Chronicles III =\\nSenjō no Valkyria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal =\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= Cicely Mary Barker =\\nCicely Mary Barker (28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>= Gambia women's national football team =\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= Plain maskray =\\nThe plain maskray or brown ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...\n",
       "1  = Tower Building of the Little Rock Arsenal =\\...\n",
       "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...\n",
       "3  = Gambia women's national football team =\\nThe...\n",
       "4  = Plain maskray =\\nThe plain maskray or brown ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d842394dca08e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:23.803702Z",
     "start_time": "2025-03-18T21:28:23.800266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29562</th>\n",
       "      <td>= Si Una Vez =\\n\"Si Una Vez\" (English: If I On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>= Sicklefin lemon shark =\\nThe sicklefin lemon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>= Flammulated flycatcher =\\nThe flammulated fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29565</th>\n",
       "      <td>= Ontario Highway 89 =\\nKing's Highway 89, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29566</th>\n",
       "      <td>= Luke Smith (writer) =\\nLuke Michael Smith is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "29562  = Si Una Vez =\\n\"Si Una Vez\" (English: If I On...\n",
       "29563  = Sicklefin lemon shark =\\nThe sicklefin lemon...\n",
       "29564  = Flammulated flycatcher =\\nThe flammulated fl...\n",
       "29565  = Ontario Highway 89 =\\nKing's Highway 89, com...\n",
       "29566  = Luke Smith (writer) =\\nLuke Michael Smith is..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15e684eba5f5845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:23.828277Z",
     "start_time": "2025-03-18T21:28:23.826013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['text'], dtype='object')\n",
      "Dataset shape: (29567, 1)\n"
     ]
    }
   ],
   "source": [
    "# Basic info\n",
    "print(\"Dataset columns:\", df_wiki.columns)\n",
    "print(\"Dataset shape:\", df_wiki.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6a5bba99f51d8",
   "metadata": {},
   "source": [
    "#### Data cleaning exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5e4b85a0259e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:23.865054Z",
     "start_time": "2025-03-18T21:28:23.852886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lenght stats:\n",
      "count     29567.000000\n",
      "mean      17537.078161\n",
      "std       14555.364685\n",
      "min          16.000000\n",
      "25%        7750.000000\n",
      "50%       12994.000000\n",
      "75%       22721.500000\n",
      "max      140098.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# text length for each entry\n",
    "df_wiki['text_length'] = df_wiki['text'].apply(lambda x: len(x))\n",
    "print(\"Text lenght stats:\")\n",
    "print(df_wiki['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1f145624cd63e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:24.442250Z",
     "start_time": "2025-03-18T21:28:23.905913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with potential quotation issues: \n",
      "                                                text  text_length\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297\n",
      "1  = Tower Building of the Little Rock Arsenal =\\...        20770\n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...        15371\n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...         6695\n",
      "5  = 2011 – 12 Columbus Blue Jackets season =\\nTh...        17189\n"
     ]
    }
   ],
   "source": [
    "# Lines with problematic formatting\n",
    "problematic = df_wiki[df_wiki['text'].str.contains(r'[\"]{1,}', na=False)]\n",
    "print(\"Rows with potential quotation issues: \") # decided to do this because of an table formatting error while doing df_wiki.head() step in the IDE.\n",
    "# The error was: “Unterminated quoted field at end of CSV line”\n",
    "print(problematic.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ccbd79adfc944",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "cleaning function to:\n",
    "- Remove HTML tags\n",
    "- Normalize Unicode to standardize characters.\n",
    "- Convert text to lowercase and remove accent marks\n",
    "- Remove non-UTF characters and extra whitespaces.\n",
    "\n",
    "This should make the text more uniform for intial tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94086e91b468bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:24.457512Z",
     "start_time": "2025-03-18T21:28:24.455511Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Normalize Unicode (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Lowercase conversion and accent stripping\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    # Remove non-UTF characters and extra whitespace\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2f052097384a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:28:57.903424Z",
     "start_time": "2025-03-18T21:28:24.468704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned sample:\n",
      "                                                text  \\\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...   \n",
      "1  = Tower Building of the Little Rock Arsenal =\\...   \n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...   \n",
      "3  = Gambia women's national football team =\\nThe...   \n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "1  = tower building of the little rock arsenal = ...  \n",
      "2  = cicely mary barker = cicely mary barker (28 ...  \n",
      "3  = gambia women's national football team = the ...  \n",
      "4  = plain maskray = the plain maskray or brown s...  \n"
     ]
    }
   ],
   "source": [
    "# Applying normalizations\n",
    "df_wiki['cleaned_text'] = df_wiki['text'].apply(normalize_text)\n",
    "print(\"cleaned sample:\")\n",
    "print(df_wiki[['text', 'cleaned_text']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea545470ae7e304c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenizing the cleaned data using Hugging Face fast tokenizer for essential downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4110d505c06390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:30:57.275571Z",
     "start_time": "2025-03-18T21:28:57.912510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5963224b85ab48a5901a9cbc33ffdf41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1297793037164bbca60e04f06bba78be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5804d3d5e26449d794efc54cde7cc7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598df0718d134d02b7f6e95678884b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4206 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "1  = tower building of the little rock arsenal = ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [=, val, ##ky, ##ria, chronicles, iii, =, sen,...  \n",
      "1  [=, tower, building, of, the, little, rock, ar...  \n"
     ]
    }
   ],
   "source": [
    "# Tokenizer initialization\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df_wiki['tokens'] = df_wiki['cleaned_text'].apply(tokenize_text)\n",
    "print(\"tokenized sample:\")\n",
    "print(df_wiki[['cleaned_text', 'tokens']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1e93a81f633e",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "Deduplication is the process of removing duplicates or near-duplicates to avoid redundancy in the dataset. For now I am choosing to remove the exact-matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "319c48482dcdfe24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:30:57.603370Z",
     "start_time": "2025-03-18T21:30:57.293030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after deduplication: 29116\n"
     ]
    }
   ],
   "source": [
    "def duplicate_texts(df, text_column = 'cleaned_text'):\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    return df\n",
    "\n",
    "df_wiki_unique = duplicate_texts(df_wiki)\n",
    "print(\"Number of rows after deduplication:\", len(df_wiki_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892177465583057",
   "metadata": {},
   "source": [
    "### Data Segmentation\n",
    "segmenting the cleaned text into sentences for finer analysis using NLTK's sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aefa5ac5b9af218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:30:57.616992Z",
     "start_time": "2025-03-18T21:30:57.614078Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_text(text, mode='sentence', fixed_token_length=100):\n",
    "    if mode == 'sentence':\n",
    "        segments = sent_tokenize(text)\n",
    "    elif mode == 'fixed':\n",
    "        tokens = tokenize_text(text)\n",
    "        segments = [' '.join(tokens[i:i+fixed_token_length]) for i in range(0, len(tokens), fixed_token_length)]\n",
    "    else:\n",
    "        segments = [text]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5e5774968bdb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:30:57.623195Z",
     "start_time": "2025-03-18T21:30:57.621263Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_dataframe(df, text_column='cleaned_text', mode='sentence'):\n",
    "    df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n",
    "    df_segmented = df.explode('segments')\n",
    "    return df_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9298c2ac5db48788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T21:31:26.936563Z",
     "start_time": "2025-03-18T21:30:57.630436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROSHAL CARDOZA\\AppData\\Local\\Temp\\ipykernel_26556\\4147776411.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented data sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "\n",
      "                                            segments  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "0  valkyria of the battlefield 3), commonly refer...  \n",
      "0  released in january 2011 in japan, it is the t...  \n",
      "0  employing the same fusion of tactical and real...  \n",
      "0  the game began development in 2010, carrying o...  \n"
     ]
    }
   ],
   "source": [
    "df_segmented = segment_dataframe(df_wiki_unique, text_column='cleaned_text', mode='sentence')\n",
    "print(\"Segmented data sample:\")\n",
    "print(df_segmented[['cleaned_text', 'segments']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0e3e70a0131d1",
   "metadata": {},
   "source": [
    "### Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb944bb1915ce4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-18T21:31:26.949155Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "output_file = os.path.join(output_dir, \"preprocessed_wikitext103.csv\")\n",
    "\n",
    "# Save the segmented DataFrame to the specified CSV file\n",
    "df_segmented.to_csv(output_file, index=False)\n",
    "print(f\"Preprocessed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b459293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries for Membership Inference Attack (MIA)\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Libraries for MIA loaded successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pre-trained model for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # CLS token representation\n",
    "\n",
    "# Convert text to embeddings (Limit to first 100 for efficiency)\n",
    "df_wiki['embedding'] = df_wiki['text'][:100].apply(get_embedding)\n",
    "\n",
    "print(\"Embeddings extracted successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split embeddings into train & attack-test sets\n",
    "X_train, X_attack_test = train_test_split(df_wiki['embedding'].tolist(), test_size=0.5, random_state=42)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = cosine_similarity(X_attack_test, X_train)\n",
    "\n",
    "# Find nearest neighbor distance\n",
    "nearest_distances = np.max(similarities, axis=1)\n",
    "\n",
    "print(\"Nearest neighbor distances computed!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f27243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate true labels (1 for in-train, 0 for out-train) for attack testing\n",
    "true_labels = np.random.choice([0, 1], size=len(X_attack_test), p=[0.5, 0.5])\n",
    "\n",
    "# Train a logistic regression attack model\n",
    "attack_model = LogisticRegression()\n",
    "attack_model.fit(nearest_distances.reshape(-1, 1), true_labels)\n",
    "\n",
    "# Evaluate the MIA attack\n",
    "attack_predictions = attack_model.predict(nearest_distances.reshape(-1, 1))\n",
    "accuracy = accuracy_score(true_labels, attack_predictions)\n",
    "\n",
    "print(f\"Membership Inference Attack(MIA) Accuracy: {accuracy:.2f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
